library(ggplot2)
library(dplyr)
library(tidyr)
library(data.table)
library(gridExtra)
library(MASS)
str(df)
df<-df %>% select(-c(X,id_num,date))
cols<-c(X,id_num,date)
cols<-c("X","id_num","date")
df<-df %>% select(-cols)
df<-df %>% select(-cols,)
df<-df %>% select(-c(X,id_num,date))
df<-read.csv("awards.csv")
str(df)
cols<-c(X,id_num,date)
library(ggplot2)
library(tidyr)
library(data.table)
library(gridExtra)
library(MASS)
library(AER)
library(dplyr)
df<-df %>% select(-c(X,id_num,date))
df<-df %>% dplyr::select(-c(X,id_num,date))
df<-df %>% dplyr::select(-c(X,id_num,date))
df<-read.csv("awards.csv")
df<-df %>% dplyr::select(-c(X,id_num,date))
library(ggplot2)
library(data.table)
library(MASS)
library(AER)
df<-read.csv("awards.csv")
str(df)
df<-df %>% dplyr::select(-c(X,id_num,date))
library(ggplot2)
library(tidyr)
library(data.table)
library(gridExtra)
library(MASS)
library(AER)
library(dplyr)
df<-read.csv("awards.csv")
str(df)
df<-df %>% dplyr::select(-c(X,id_num,date))
#sapply(df, function(x) sum(is.na(x)))
#summary(df)
df<-separate(df,school.prog,c("School", "Program"),sep="/")
df$imp<-ordered(df$imp,levels=c(1,2,3,4),labels=c('Not important','Neutral','Important','Very important'))
df$School<-factor(df$School)
df$Program<-factor(df$Program,levels = c(0,1,2,3),labels = c("General","Pre-Academic","Academic","Vocational"))
df<-df[!df$hpw<0,]
glimpse(df)
#summary(df[which(unlist(lapply(df,is.numeric)))])
ggplot(df,aes(awards))+geom_histogram( col="red", fill="green", alpha=.2)+ggtitle("Histogram of Awards")
var(df$awards)
mean(df$awards)
g1=ggplot(data = df, aes(x = gender, y = awards,color=gender))+
geom_boxplot()+
scale_x_discrete(labels= levels(df$gender))+
stat_summary(fun.y = "mean",geom = 'point',col='red', shape=17, size=4)+
labs(title ="Relationship between Awards and Gender" ,y='Awards',x="Gender",color="Gebder")+theme(plot.title = element_text(hjust = 0.5))
g2 = ggplot(data = df, aes(x = imp, y = awards,color = imp))+
geom_boxplot()+
scale_x_discrete(labels= levels(df$imp) )+
stat_summary(fun.y = "mean",geom = 'point',col='blue', shape=12, size=4)+
labs(title ="Relationship between Awards and Importance" ,color =  "Awards",y='Awards',x="Importance of getting an award.")+theme(plot.title = element_text(hjust = 0.5))
g3<-ggplot(data = df, aes(x = Program, y = awards,color = Program))+
geom_boxplot()+facet_grid(.~School)+
scale_x_discrete(labels= levels(df$Program) )+
stat_summary(fun.y = "mean",geom = 'point',col='blue', shape=12, size=4)+
labs(title ="Relationship between Awards and Program by School" ,color =  "Program",y='Awards',x="Program")+theme(axis.text.x = element_text(angle = 90, hjust = 1))
grid.arrange(g1, g2,g3,nrow=2)
colnames(df)
df%>%group_by(gender)%>%
summarise(var=var(awards),mean=mean(awards))
df%>%group_by(imp)%>%
summarise(var=var(awards),mean=mean(awards))
df%>%group_by(School)%>%
summarise(var=var(awards),mean=mean(awards))
df%>%group_by(Program)%>%
summarise(var=var(awards),mean=mean(awards))
library(ISLR)
library(car)
### Review before the midterm exam###
library(ISLR)
library(car)
data(package = "ISLR")
help("Hitters")
# We wish to predict salary on the basis of other predictors from Hitters data set.
fix(Hitters)
### We note that the Salary variable is missing for some of the players.
dim(Hitters)
sum(is.na(Hitters$Salary))
### is.na() identifing the missing observations.
Hitters=na.omit(Hitters)
### na.omit() removes all the rows that have missing value.
dim(Hitters)
sum(is.na(Hitters))
attach(Hitters)
names(Hitters)
### Multiple Linear Regression ###
lm1=lm(Salary~., data = Hitters)
summary(lm1)
lm2=lm(Salary~AtBat+Hits+Walks+CRuns+CWalks+Division+PutOuts+Assists, data = Hitters)
summary(lm2)
lm3=lm(Salary~AtBat+Hits+Walks+CRuns+CWalks+Division+PutOuts, data = Hitters)
summary(lm3)
AIC(lm1, lm3)
BIC(lm1, lm3)
### adjusted R^2.
hist(Salary)
hist(log(Salary))
hist(AtBat)
hist(Hits)
hist(Walks)
hist(CRuns)
hist(log(CRuns))
hist(CWalks)
hist(log(CWalks))
Hitters$logSalary=log(Hitters$Salary)
Hitters$logCRuns=log(Hitters$CRuns)
Hitters$logCWalks=log(Hitters$CWalks)
lm4=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm4)
AIC(lm3, lm4)
BIC(lm3, lm4)
### adjusted R^2.
plot(lm4, which=c(1))
plot(lm4, which=c(1))
plot(lm4, which=c(2))
plot(lm4, which=c(1))
plot(lm4, which=c(2))
plot(lm4, which=c(3))
plot(lm4, which=c(5))
plot(predict(lm4), rstudent(lm4))
### Studentized Residuals vs Fitted values
abline(h = -3, col = 'red')
abline(h = 3, col = 'red')
which(abs(rstudent(lm4))>3)
hatvalues(lm4)
plot(hatvalues(lm4))
abline(h=2*8/nrow(Hitters), col = 'red')
which(hatvalues(lm3)>2*8/nrow(Hitters))
plot(hatvalues(lm4))
abline(h=3*8/nrow(Hitters), col = 'red')
vif(lm4)
vif(lm4)
HittersHitters[c(173,230,241),]
Hitters[c(173,230,241),]
Hitters[,c(173,230,241)]
Hitters[c(173,230,241),]
vif(lm4)
lm6=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+Division+PutOuts, data = Hitters)
summary(lm6)
vif(lm4)
plot(hatvalues(lm4))
plot(predict(lm4), rstudent(lm4))
### Studentized Residuals vs Fitted values
abline(h = -3, col = 'red')
abline(h = 3, col = 'red')
which(abs(rstudent(lm4))>3)
hatvalues(lm4)
plot(hatvalues(lm4))
plot(lm4, which=c(2))
plot(lm4, which=c(3))
plot(lm4, which=c(5))
Hitters=Hitters[-c(173,230,241),]
Hitters=Hitters[-c(173,230,241),]
Hitters[-c(173,230,241),]
library(ISLR)
library(car)
data(package = "ISLR")
#help("Hitters")
# We wish to predict salary on the basis of other predictors from Hitters data set.
#fix(Hitters)
### We note that the Salary variable is missing for some of the players.
#dim(Hitters)
sum(is.na(Hitters$Salary))
### is.na() identifing the missing observations.
#Hitters=na.omit(Hitters)
### na.omit() removes all the rows that have missing value.
dim(Hitters)
sum(is.na(Hitters))
#attach(Hitters)
names(Hitters)
### Multiple Linear Regression ###
lm1=lm(Salary~., data = Hitters)
summary(lm1)
lm2=lm(Salary~AtBat+Hits+Walks+CRuns+CWalks+Division+PutOuts+Assists, data = Hitters)
summary(lm2)
lm3=lm(Salary~AtBat+Hits+Walks+CRuns+CWalks+Division+PutOuts, data = Hitters)
summary(lm3)
AIC(lm1, lm3)
BIC(lm1, lm3)
### adjusted R^2.
hist(Salary)
hist(log(Salary))
hist(AtBat)
hist(Hits)
hist(Walks)
hist(CRuns)
hist(log(CRuns))
hist(CWalks)
hist(log(CWalks))
Hitters$logSalary=log(Hitters$Salary)
Hitters$logCRuns=log(Hitters$CRuns)
Hitters$logCWalks=log(Hitters$CWalks)
lm4=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm4)
AIC(lm3, lm4)
BIC(lm3, lm4)
### adjusted R^2.
plot(lm4, which=c(1))
### Residuals vs Fitted values. This plot shows if residuals have non-linear patterns.
### There could be a non-linear relationship between predictor variables
### and an outcome variable and the pattern could show up in this plot
### if the model doesn't capture the non-linear relationship.
### If you find equally spread residuals around a horizontal line without distinct
### patterns, that is a good indication you don't have non-linear relationships.
plot(lm4, which=c(2))
### Normal Q-Q. This plot shows if residuals are normally distributed.
### Do residuals follow a straight line well or do they deviate severely?
### It's good if residuals are lined well on the straight dashed line.
plot(lm4, which=c(3))
### sqrt(|Standardized Residuals|) vs Fitted values.
### This plot shows if residuals are spread equally along the ranges of predictors.
### This is how you can check the assumption of constant variance of error terms.
### It's good if you see a horizontal line with equally (randomly) spread points.
plot(lm4, which=c(5))
### Residuals vs Leverage. This plot helps us to find influential cases if any.
### We watch out for outlying values at the upper right corner or at the lower right corner.
### Those spots are the places where cases can be influential against a regression line.
### Look for cases outside of a dashed line, Cook's distance.
### When cases are outside of the Cook's distance (meaning they have high Cook's distance
### scores), the cases are influential to the regression results.
### The regression results will be altered if we exclude t
plot(lm4, which=c(1))
plot(lm4, which=c(2))
plot(lm4, which=c(3))
plot(lm4, which=c(5))
plot(predict(lm4), rstudent(lm4))
plot(hatvalues(lm4))
abline(h=2*8/nrow(Hitters), col = 'red')
which(hatvalues(lm3)>2*8/nrow(Hitters))
plot(hatvalues(lm4))
abline(h=3*8/nrow(Hitters), col = 'red')
which(hatvalues(lm4)>3*8/nrow(Hitters))
vif(lm4)
which(hatvalues(lm4)>3*8/nrow(Hitters))
which(hatvalues(lm3)>2*8/nrow(Hitters))
which(hatvalues(lm4)>3*8/nrow(Hitters))
Hitters[62,]
which(hatvalues(lm4)>3*8/nrow(Hitters))
Hitters[-c(62,169,250),]
Hitters=Hitters[-c(173,230,241),]
plot(hatvalues(lm4))
abline(h=3*8/nrow(Hitters), col = 'red')
which(hatvalues(lm4)>3*8/nrow(Hitters))
lm5=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm5)
plot(hatvalues(lm5))
abline(h=3*8/nrow(Hitters), col = 'red')
which(hatvalues(lm4)>3*8/nrow(Hitters))
Hitters=Hitters[-c(62,169,250),]
lm5=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm5)
plot(hatvalues(lm5))
abline(h=3*8/nrow(Hitters), col = 'red')
which(hatvalues(lm4)>3*8/nrow(Hitters))
which(hatvalues(lm4)>3*8/nrow(Hitters))
which(hatvalues(lm5)>3*8/nrow(Hitters))
mean(Hitters$logSalary)
0.4382/5.922586
confint(lm6)
rm(list=ls())
confint(lm6)
lm6=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+Division+PutOuts, data = Hitters)
summary(lm6)
lm6=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+Division+PutOuts, data = Hitters)
Hitters$logSalary=log(Hitters$Salary)
Hitters$logCRuns=log(Hitters$CRuns)
Hitters$logCWalks=log(Hitters$CWalks)
lm4=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm4)
AIC(lm3, lm4)
BIC(lm3, lm4)
### adjusted R^2.
plot(lm4, which=c(1))
### Residuals vs Fitted values. This plot shows if residuals have non-linear patterns.
### There could be a non-linear relationship between predictor variables
### and an outcome variable and the pattern could show up in this plot
### if the model doesn't capture the non-linear relationship.
### If you find equally spread residuals around a horizontal line without distinct
### patterns, that is a good indication you don't have non-linear relationships.
plot(lm4, which=c(2))
### Normal Q-Q. This plot shows if residuals are normally distributed.
### Do residuals follow a straight line well or do they deviate severely?
### It's good if residuals are lined well on the straight dashed line.
plot(lm4, which=c(3))
### sqrt(|Standardized Residuals|) vs Fitted values.
### This plot shows if residuals are spread equally along the ranges of predictors.
### This is how you can check the assumption of constant variance of error terms.
### It's good if you see a horizontal line with equally (randomly) spread points.
plot(lm4, which=c(5))
### Residuals vs Leverage. This plot helps us to find influential cases if any.
### We watch out for outlying values at the upper right corner or at the lower right corner.
### Those spots are the places where cases can be influential against a regression line.
### Look for cases outside of a dashed line, Cook's distance.
### When cases are outside of the Cook's distance (meaning they have high Cook's distance
### scores), the cases are influential to the regression results.
### The regression results will be altered if we exclude those cases.
plot(predict(lm4), rstudent(lm4))
### Studentized Residuals vs Fitted values
abline(h = -3, col = 'red')
abline(h = 3, col = 'red')
which(abs(rstudent(lm4))>3)
### for identifying outliers.
hatvalues(lm4)
# Leverage statistics.
plot(hatvalues(lm4))
abline(h=2*8/nrow(Hitters), col = 'red')
which(hatvalues(lm3)>2*8/nrow(Hitters))
### Use 2(p+1)/n as a threshold to determine high leverage points.
### or
plot(hatvalues(lm5))
abline(h=3*8/nrow(Hitters), col = 'red')
which(hatvalues(lm5)>3*8/nrow(Hitters))
### Use 3(p+1)/n as a threshold to determine high leverage points.
vif(lm4)
Hitters=Hitters[-c(62,169,250),]
Hitters=Hitters[-c(173,230,241),]
lm5=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm5)
lm6=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+Division+PutOuts, data = Hitters)
summary(lm6)
### F-stat.=132.5>>1 and corresponding p-value<2.2*10^{-16} implies that there is a relationship
### between Salary and the other variables.
### R^2=0.7586, it means that the 75.86% variability of the logSalary can be explained by these variables.
mean(Hitters$logSalary)
0.4382/5.922586
mean(Hitters$logSalary)
grid=10^seq(10,-2,length=100)
ridge_mod=glmnet(x,y,alpha=0,lambda=grid)
### Review before the midterm exam###
library(ISLR)
library(car)
data(package = "ISLR")
#help("Hitters")
# We wish to predict salary on the basis of other predictors from Hitters data set.
#fix(Hitters)
### We note that the Salary variable is missing for some of the players.
#dim(Hitters)
sum(is.na(Hitters$Salary))
### is.na() identifing the missing observations.
#Hitters=na.omit(Hitters)
### na.omit() removes all the rows that have missing value.
dim(Hitters)
sum(is.na(Hitters))
#attach(Hitters)
names(Hitters)
### Multiple Linear Regression ###
lm1=lm(Salary~., data = Hitters)
summary(lm1)
lm2=lm(Salary~AtBat+Hits+Walks+CRuns+CWalks+Division+PutOuts+Assists, data = Hitters)
summary(lm2)
lm3=lm(Salary~AtBat+Hits+Walks+CRuns+CWalks+Division+PutOuts, data = Hitters)
summary(lm3)
AIC(lm1, lm3)
BIC(lm1, lm3)
### adjusted R^2.
hist(Salary)
hist(log(Salary))
hist(AtBat)
hist(Hits)
hist(Walks)
hist(CRuns)
hist(log(CRuns))
hist(CWalks)
hist(log(CWalks))
Hitters$logSalary=log(Hitters$Salary)
Hitters$logCRuns=log(Hitters$CRuns)
Hitters$logCWalks=log(Hitters$CWalks)
lm4=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm4)
AIC(lm3, lm4)
BIC(lm3, lm4)
### adjusted R^2.
plot(lm4, which=c(1))
### Residuals vs Fitted values. This plot shows if residuals have non-linear patterns.
### There could be a non-linear relationship between predictor variables
### and an outcome variable and the pattern could show up in this plot
### if the model doesn't capture the non-linear relationship.
### If you find equally spread residuals around a horizontal line without distinct
### patterns, that is a good indication you don't have non-linear relationships.
plot(lm4, which=c(2))
### Normal Q-Q. This plot shows if residuals are normally distributed.
### Do residuals follow a straight line well or do they deviate severely?
### It's good if residuals are lined well on the straight dashed line.
plot(lm4, which=c(3))
### sqrt(|Standardized Residuals|) vs Fitted values.
### This plot shows if residuals are spread equally along the ranges of predictors.
### This is how you can check the assumption of constant variance of error terms.
### It's good if you see a horizontal line with equally (randomly) spread points.
plot(lm4, which=c(5))
### Residuals vs Leverage. This plot helps us to find influential cases if any.
### We watch out for outlying values at the upper right corner or at the lower right corner.
### Those spots are the places where cases can be influential against a regression line.
### Look for cases outside of a dashed line, Cook's distance.
### When cases are outside of the Cook's distance (meaning they have high Cook's distance
### scores), the cases are influential to the regression results.
### The regression results will be altered if we exclude those cases.
plot(predict(lm4), rstudent(lm4))
### Studentized Residuals vs Fitted values
abline(h = -3, col = 'red')
abline(h = 3, col = 'red')
which(abs(rstudent(lm4))>3)
### for identifying outliers.
hatvalues(lm4)
# Leverage statistics.
plot(hatvalues(lm4))
abline(h=2*8/nrow(Hitters), col = 'red')
which(hatvalues(lm3)>2*8/nrow(Hitters))
### Use 2(p+1)/n as a threshold to determine high leverage points.
### or
plot(hatvalues(lm5))
abline(h=3*8/nrow(Hitters), col = 'red')
which(hatvalues(lm5)>3*8/nrow(Hitters))
### Use 3(p+1)/n as a threshold to determine high leverage points.
vif(lm4)
Hitters=Hitters[-c(62,169,250),]
Hitters=Hitters[-c(173,230,241),]
lm5=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+logCWalks+Division+PutOuts, data = Hitters)
summary(lm5)
lm6=lm(logSalary ~ AtBat+Hits+Walks+logCRuns+Division+PutOuts, data = Hitters)
summary(lm6)
### F-stat.=132.5>>1 and corresponding p-value<2.2*10^{-16} implies that there is a relationship
### between Salary and the other variables.
### R^2=0.7586, it means that the 75.86% variability of the logSalary can be explained by these variables.
mean(Hitters$logSalary)
0.4382/5.922586
### RSE/mean(Salary)=0.074 or actual logSalary deviate from the true regression line by 0.44 units or by 7.4% on average.
confint(lm6)
### Notice that the confidence interval for Division includes zero,
### indicating that this variable is not statistically significant.
### Ridge Regression ###
rm(list=ls())
#install.packages("glmnet")
library(glmnet)
Hitters=na.omit(Hitters)
x=model.matrix(Salary~.,Hitters)[,-1]
### Create a matrix corresponding to the 19 predictors and automatically
### transform any qualitative variables into dummy variables
y=Hitters$Salary
### Compare to see the difference for qualitative variables.
x[1:2,]
Hitters[1:2,]
grid=10^seq(10,-2,length=100)
### Creating a grid of values ranging from \lambda=10^10 to 10^(-2)
ridge_mod=glmnet(x,y,alpha=0,lambda=grid)
### \alpha=0 for ridge regression, \alpha=1 for the Lasso
dim(coef(ridge_mod))
dim(coef(ridge_mod))
ridge_mod$lambda[50]
### \lambda=11497.57
coef(ridge_mod)[,50]
sqrt(sum(coef(ridge_mod)[-1,50]^2))
ridge_mod$lambda[60]
### \lambda=705.48
coef(ridge_mod)[,60]
sqrt(sum(coef(ridge_mod)[-1,60]^2))
predict(ridge_mod,s=50,type="coefficients")[1:20,]
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y_test=y[test]
ridge_mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge_pred=predict(ridge_mod,s=4,newx=x[test,])
mean((ridge_pred-y_test)^2)
ridge_pred=predict(ridge_mod,s=1e10,newx=x[test,])
mean((ridge_pred-y_test)^2)
mean((ridge_pred-y_test)^2)
lm(y~x, subset=train)
lm(y~x, subset=train)
predict(ridge_mod,s=0,exact=T,type="coefficients",x=x[train,],y=y[train])[1:20,]
library(Metrics)
